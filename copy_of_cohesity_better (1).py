# -*- coding: utf-8 -*-
"""Copy of cohesity_better.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jKb2Q9Kxl76KSgvNKs_WEfTxvWYczXpj
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import numpy as np
# import re
# import pandas as pd
# !pip install nltk
# !pip install summa
# !pip install wordninja
# from summa import keywords
# import re
# import nltk
# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('wordnet')
# nltk.download('omw-1.4')
# import wordninja as ninja
# from nltk.stem import WordNetLemmatizer
# from nltk.stem import PorterStemmer
# from nltk.tokenize import word_tokenize,sent_tokenize
# !pip install sentence_transformers
# from sentence_transformers import SentenceTransformer

def clean(query):
  query = re.sub("[^a-zA-Z :\.]", "",query)                         # remove all symbols except .
  
  new_word = []
  for word in nltk.word_tokenize(query):
    if word.lower().strip() != "cohesity": 
      new_word.append(word.strip().lower())
  
  if len(new_word) > 2:
    return ' '.join(new_word)
  return False


def ninja_technique(query):
  return " ".join(ninja.split(query))


wordnet_lemmatizer = WordNetLemmatizer()
def lemmatize(query):
    new_word = []
    for word in query.split():
        new_word.append(wordnet_lemmatizer.lemmatize(word, pos="v").strip())

    return ' '.join(new_word)
 

ps = PorterStemmer()
def stem_(query):
    new_word = []
    for word in query.split():
        new_word.append(ps.stem(word).strip())

    return ' '.join(new_word)

"""Total Case Created     No clicks case creation"""
no_clicks = pd.read_excel("Cohesity Dataset to be worked upon(Jul - Aug).xlsx",sheet_name="Total Case Created", usecols=["Activity Detail"])
print(len(no_clicks))
no_clicks.drop_duplicates(inplace=True)
print(len(no_clicks))

case_ids = []
for row in no_clicks["Activity Detail"].to_list():
  id = row[-8:]
  if id.isdigit():
    case_ids.append(row[-8:])
  else:
    case_ids.append("-")

no_clicks["CASE_ID"] = case_ids

# lower + remove word length <=2 
no_clicks["Activity Detail"] = no_clicks["Activity Detail"].apply(clean)   
no_clicks = no_clicks[no_clicks["Activity Detail"] != False]
print(len(no_clicks))

# split joined words 
no_clicks["clean"] = no_clicks["Activity Detail"].apply(ninja_technique)
print(len(no_clicks))

# lemmatize and stem
# no_clicks["clean"] = no_clicks["clean"].apply(lemmatize) 
# no_clicks["clean"] = no_clicks["clean"].apply(stem_)

# sample = no_clicks.sample(1000,random_state=694)
sample = no_clicks

sample

# %%capture
# !pip install keybert
# from keybert import KeyBERT

# kw_model = KeyBERT(model='all-mpnet-base-v2')

# def get_keywords(title):
#   keywords = kw_model.extract_keywords(title, keyphrase_ngram_range=(3, 3), stop_words='english', highlight=False, top_n=2)
#   keywords_list= list(dict(keywords).keys())
#   return "_".join(set(" ".join(keywords_list).split()))

# sample["topic"] = sample["clean"].apply(get_keywords)

# sample

model = SentenceTransformer('all-mpnet-base-v2')
# model = SentenceTransformer('bert-base-nli-mean-tokens')
encoding_ = model.encode(sample['clean'].to_list())
data_encoding = np.array(encoding_)
np.save('encoding.npy', data_encoding)
embedding = np.load('encoding.npy')

embedding.shape

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install nmslib
# import nmslib
# 
# class NMSLIBIndex():
#   def __init__(self, vectors, labels):
#     self.dimention = vectors.shape[0]
#     self.vectors = vectors.astype('float32')
#     self.labels = labels
#   
#   def build(self):
#     self.index = nmslib.init(method='hnsw', space='cosinesimil')
#     self.index.addDataPointBatch(self.vectors)
#     self.index.createIndex({'post': 2})
#   
#   # find 50% nearest neigbours (half to the dataset)
#   def query(self, vector, k):
#     indices = self.index.knnQuery(vector, k=k)
#     return [self.labels[i] for i in indices[0]], [round(i,2) for i in indices[1]]

vector_list = []
for emb in embedding:
  vector_list.append(emb)

vect = np.array(vector_list)

new_df = pd.DataFrame({"vector" : vector_list, "title" : sample["clean"], "CASE_ID":sample["CASE_ID"]})
new_df = new_df.reset_index()
new_df.drop(columns=["index"],inplace=True)
new_df

new_df[new_df["title"] == "ce lond bk ch frequent process restarts"]["CASE_ID"].values[0]

nm_index = NMSLIBIndex(vect, new_df["title"])
nm_index.build()

K = embedding.shape[0]//2

pd_dict = {
    "main_title" : [],
    "similar_titles" : [],
    "distance" : [],
    "group" : [],
    "case" : []
}

group = 0

for vector,label in zip(new_df['vector'],new_df['title']) :
  queries_nm, distances_nm = nm_index.query(vector,K) 
  # small_list = []                                        # put all same queries in this list then we take common word as group  
  for q,d in zip(queries_nm, distances_nm):
    # if label != q:
    if d < 0.2:
      pd_dict["main_title"].append(label)
      pd_dict["similar_titles"].append(q)
      pd_dict["distance"].append(d)
      pd_dict["group"].append(group)
      case = new_df[new_df["title"] == q]["CASE_ID"].values[0]
      pd_dict["case"].append(case)

    # small_list.append(q)
  # find common words in this group of same queries  
  print(group,"--------")
  # group = "_".join(set(" ".join(keywords_list).split()))
  group +=1

frame = pd.DataFrame(pd_dict)
frame.drop_duplicates(subset="similar_titles",inplace=True)
frame.to_excel("(k)Total_cases_cohesity_with_caseID(more_similar).xlsx")
frame.head(50)

# doc2vec

frame.groupby(by=["group"],as_index=False).count().sort_values(by="distance",ascending=False)

frame["similar_titles"].nunique()

len(frame[frame["group"] == 12])

len(frame)

(len(frame[frame["group"] == 12])/len(frame)) * 100

"""

top group (12) has 93 similar queries inside, which is 1.92% of total 4840 rows

"""

for g in set(frame["group"].to_list()):
  queries_in_group = len(frame[frame["group"] == g])
  percentage = (queries_in_group/len(frame)) * 100
  if percentage > 0.75:
    total = len(frame)
    print(f"Group - {g}, has {queries_in_group} queries in it, which is {round(percentage,2)}% of the total {total} queries.")

